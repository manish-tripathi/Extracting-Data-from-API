# About Me

Hey there, I’m Manish Tripathi and I love crunching numbers and finding insights. I work as Lead Market Analyst at SkillGigs.com, where I help tech talent land their dream jobs. Before that, I was a Research Associate at S&P Global for 11 years, doing research for different sectors and markets and improving data operations.

I have a Post Graduate Diploma in Management from Institite of Management Technology, Nagpur, with a focus on Finance and Human Resource. I also have a Bachelor of Mathematics degree from St. Xavier's College Mumbai.

But enough about my education and work experience. What really excites me is learning new skills and technologies, especially in the field of natural language processing and machine learning. I have taken several online courses and certifications on these topics. I also enjoy reading technical books and articles on these topics. You can check out some of my publications on my [personal website](https://manish-tripathi.github.io/Projects/) or on my [LinkedIn profile](https://www.linkedin.com/in/maneeshtripathi/).

# My Projects

# [Project 1: Python Customer Segmentation & Clustering](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/Python%20Customer%20Segmentation%20%26%20Clustering.ipynb)
Attempted K-Means Clustering and Agglomerative Clustering to segment customer Annual Income and Spending Score data\
[Dataset Link](https://raw.githubusercontent.com/Gaelim/Mall-Customer-Segmentation/main/Mall_Customers.csv)\
[Notebook Motivation and dataset provided by](https://www.youtube.com/watch?v=iwUli5gIcU0)


# [Project 2: Extracting Data from API](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/NewsApi_Manish_Tripathi.ipynb)
The Objective of the project is to fetch News using API from https://newsapi.org/

# [Project 3: Scrapping Naukri.com for Information Technology Jobs posted in Hyderabad](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/Naukri_Webscrapper.ipynb)
The first step in any data science job is to get Data. Collecting Primary data from respondants is costly and time consuming. One of the ways we can collecta dat is through scrapping it from the internet. The aim of this project is to identify key companies hiring for Information Technology Roles in Hyderabad and identify the skills they are looking for. For the project, have scrapped [Naukri.com](https://www.naukri.com/information-technology-jobs-in-hyderabad-secunderabad) for collecting the data.\
[Output Dataset](https://github.com/manish-tripathi/Projects/blob/main/Hyderabad_Information_Technology_Jobs.csv)

# [Project 4: Classifying if Water is Safe for Drinking using XGBoost](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/classifying-unbalanced-dataset-using-xgboost.ipynb)
There are multiple scenarios where we need to identify the category of a given dataset; for example:

* Classifying emails as spam or not
* Classify a given handwritten character to be either a known character or not
* Classify recent user behaviour as churn or not

XGBoost is considered to be one of the most powerful models for classification. We’ll see how it can be applied to classification problems. In this implementation, we are going to apply the XGBoost classifier to address the binary classification problem where our task is to predict whether water is safe for drinking or not.

The dataset holds information about the level of various ingredients present in water such as copper, lead, mercury, etc and there are such 20 features.

![image](https://user-images.githubusercontent.com/8171780/163837783-7049b2e4-0152-4708-8e68-8558f20372cd.png)

In binary features or the target feature, 0 states that the water is safe for drinking, and 1 states that it is not safe for drinking. So by making use of these features we are going to build an XGBoost classifier that will fit the relationship between those 20 features and the target feature.

[Learn More About the project](https://machinehack.com/bootcamp/bootcampcourse/623c3b39473fe2338d71edad)


# [Project 5: Natural Language Processing Using Hugging Face Library](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/nlp-using-hugging-face-library.ipynb)

Who thought learning Natural Language Processing could be so easy.
Started learning Natural Language Processing using Natural Language Processing with Transformers.
Transformers help in solving complex tasks like sentiment analysis, named entity recognition, translation, summarization, question answering and text generation with single line of code.
To learn more refer: [Natural Language Processing with Transformers](https://www.amazon.in/Natural-Language-Processing-Transformers-Applications/dp/9355421877/ref=asc_df_9355421877/?tag=googleshopdes-21&linkCode=df0&hvadid=586323570422&hvpos=&hvnetw=g&hvrand=7312920761120531252&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1007819&hvtargid=pla-1637749518926&psc=1)
The code for the book is freely avaiable on [Github](https://github.com/nlp-with-transformers/notebooks)


# [Project 6: Create custom Datasets from Wikipedia with Python](https://nbviewer.org/github/manish-tripathi/Projects/blob/main/create-custom-datasets-from-wikipedia-with-python.ipynb)
The basic requirement for any data science project is **Data**. In this notebook, I will be following along how to create custom datasets from wikipedia using python and pandas using pandas read_html function. Video tutorial for the same was posted [Rob Mulla, Kaggle Grandmaster](https://www.kaggle.com/robikscube), on his [Youtube Channel](https://youtu.be/KokJHxiE14s). The method described makes reading tables from html and websites really easy.

[Follow me on LinkedIn](https://www.linkedin.com/mynetwork/discovery-see-all/?usecase=PEOPLE_FOLLOWS&followMember=maneeshtripathi)
